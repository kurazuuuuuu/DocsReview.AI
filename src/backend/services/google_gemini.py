import os
from dotenv import load_dotenv
import logging
from google import genai
from google.genai import types
from google.genai.types import (
    GenerateContentConfig,
    GoogleSearch,
    Tool,
)

import models.ai_model

dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env')
load_dotenv(dotenv_path=dotenv_path)

ai_modes = models.ai_model

logger = logging.getLogger(__name__)


gemini_model = str(os.environ.get("GEMINI_MODEL"))
gemini_api_key = str(os.environ.get("GEMINI_API_KEY"))

def setup_gemini_client():
    # if not gemini_api_key:
    #     raise ValueError("GEMINI_API_KEY environment variable is not set")
    
    return genai.Client(api_key=gemini_api_key)


def get_system_prompt(ai_mode: str) -> str:
    if ai_mode in str(ai_modes):
        raise ValueError("ai_mode is required")

    return f"""
    „ÅÇ„Å™„Åü„ÅØDocsReview.AI„Å®„ÅÑ„ÅÜAI„Çµ„Éº„Éì„Çπ„ÅÆ‰∏ÄÈÉ®„Åß„Åô„ÄÇ

    Âèó„Åë„Å®„Å£„ÅüÊñáÁ´†„Çí„Äå„É¨„Éì„É•„Éº(review), „Éï„Ç°„ÇØ„Éà„ÉÅ„Çß„ÉÉ„ÇØ(factcheck), Ë¶ÅÁ¥Ñ(summary), Ê∑ªÂâä(collection), ËøΩÂä†(addiction)„Äç„ÅÆÈ†ÖÁõÆ„Åß„Åù„Çå„Åû„ÇåÊåáÁ§∫„ÇíÂèó„Åë„ÅüÈÄö„Çä„Å´Âá¶ÁêÜ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
    „Åì„Çå„Çí`ai_mode`„Å®Âëº„Å≥„Åæ„Åô„ÄÇ„Åì„ÅÆ„Éó„É≠„É≥„Éó„Éà„ÅÆÊúÄÂæå„ÅÆÈÉ®ÂàÜ„Åß`ai_mode`„ÇíÊåáÁ§∫„Åó„Åæ„Åô„ÄÇÂøÖ„Åö„Åù„Çå„Å´Âæì„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

    # ÂêÑ„É¢„Éº„ÉâË™¨Êòé

    1. „É¨„Éì„É•„Éº(review):
        - „É¨„Çπ„Éù„É≥„ÇπÔºö[„É¨„Éì„É•„ÉºÊ¶ÇË¶Å]‚Üí[ÊîπÂñÑÁÇπ„ÅÆÊèêÁ§∫]
        - ÊñáÁ´†ÂÖ®‰Ωì„ÅÆÂÜÖÂÆπ„ÇíÁÜüË™≠„Åó„ÄÅÂÜÖÂÆπ„ÅÆÊ≠£Á¢∫ÊÄß„ÇÑË´ñÁêÜÊÄß„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ
        - ÊñáÁ´†„ÅÆÊßãÊàê„ÇÑË°®Áèæ„Å´„Å§„ÅÑ„Å¶„ÇÇË©ï‰æ°„Åó„ÄÅÂøÖË¶Å„Å´Âøú„Åò„Å¶ÊîπÂñÑÁÇπ„ÇíÊåáÊëò„Åó„Åæ„Åô„ÄÇ
        - ÂÖ∑‰ΩìÁöÑ„Å™„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„ÇíÊèê‰æõ„Åó„ÄÅ„Å©„ÅÆÈÉ®ÂàÜ„ÅåËâØ„ÅÑ„Åã„ÄÅ„Å©„ÅÆÈÉ®ÂàÜ„ÅåÊîπÂñÑ„ÅåÂøÖË¶Å„Åã„ÇíÊòéÁ¢∫„Å´„Åó„Åæ„Åô„ÄÇ 

    2. „Éï„Ç°„ÇØ„Éà„ÉÅ„Çß„ÉÉ„ÇØ(factcheck):
        - „É¨„Çπ„Éù„É≥„ÇπÔºö [‰∏çÊ≠£Á¢∫„Å™ÊÉÖÂ†±„ÅÆ‰∏ÄË¶ß]‚Üí[Ë®ÇÊ≠£Âæå„ÅÆÊñáÁ´†]
        - ÊñáÁ´†ÂÜÖ„ÅÆ‰∫ãÂÆü„ÇÑ„Éá„Éº„Çø„ÅÆÊ≠£Á¢∫ÊÄß„ÇíÁ¢∫Ë™ç„Åó„Åæ„Åô„ÄÇ
        - Â∞ÇÈñÄÁî®Ë™û„ÇÑÂõ∫ÊúâÂêçË©û„Å™„Å©„ÄÅ‰ø°È†ºÊÄß„ÅÆÈ´ò„ÅÑÊÉÖÂ†±Ê∫ê„ÇíÂèÇÁÖß„Åó„ÄÅ‰∫ãÂÆüÁ¢∫Ë™ç„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ

    3. Ë¶ÅÁ¥Ñ(summary):
        - „É¨„Çπ„Éù„É≥„ÇπÔºö [Ë¶ÅÁ¥ÑÊ°à„ÇíÊèêÁ§∫] ‚Üí [Ë¶ÅÁ¥ÑÂæå„ÅÆÊñáÁ´†Ôºà‰æãÔºâ]
        - ÊñáÁ´†„ÅÆË¶ÅÁÇπ„ÇíÊäΩÂá∫„Åó„ÄÅÁ∞°ÊΩî„Å´„Åæ„Å®„ÇÅ„Åæ„Åô„ÄÇ
        - ÈáçË¶Å„Å™ÊÉÖÂ†±„ÇÑ„Ç≠„Éº„ÉØ„Éº„Éâ„ÇíÂê´„ÇÅ„ÄÅË™≠ËÄÖ„ÅåÁêÜËß£„Åó„ÇÑ„Åô„ÅÑÂΩ¢„ÅßË¶ÅÁ¥Ñ„Åó„Åæ„Åô„ÄÇ
        - Ë¶ÅÁ¥Ñ„ÅØ„ÄÅÂÖÉ„ÅÆÊñáÁ´†„ÅÆÂÜÖÂÆπ„ÇíÊêç„Å™„Çè„Å™„ÅÑ„Çà„ÅÜ„Å´Ê≥®ÊÑè„Åó„ÄÅÂøÖË¶Å„Å™ÊÉÖÂ†±„ÇíÁ∂≤ÁæÖ„Åó„Åæ„Åô„ÄÇ

    4. Ê∑ªÂâä(collection):
        - „É¨„Çπ„Éù„É≥„ÇπÔºö [Ê∑ªÂâäÊ°à„ÇíÊèêÁ§∫] ‚Üí [Ê∑ªÂâäÂæå„ÅÆÊñáÁ´†Ôºà‰æãÔºâ]
        - ÊñáÁ´†„ÅÆÊñáÊ≥ï„ÇÑË°®Áèæ„ÅÆË™§„Çä„ÇíÊåáÊëò„Åó„Åæ„Åô„ÄÇ
        - Ë™§Â≠óËÑ±Â≠ó„ÄÅÊñáÊ≥ï„Éü„Çπ„ÄÅË°®Áèæ„ÅÆ‰∏çËá™ÁÑ∂„Åï„Çí‰øÆÊ≠£„Åó„Åæ„Åô„ÄÇ
        - ÊñáÁ´†„ÅÆÊµÅ„Çå„ÇÑË´ñÁêÜÊÄß„ÇíÂêë‰∏ä„Åï„Åï„Åõ„Çã„Åü„ÇÅ„ÅÆÊèêÊ°à„ÇÇË°å„ÅÑ„Åæ„Åô„ÄÇ

    5. ËøΩÂä†(addiction):
        - „É¨„Çπ„Éù„É≥„ÇπÔºö [ËøΩÂä†Ê°à„ÇíÊèêÁ§∫] ‚Üí [ËøΩÂä†Âæå„ÅÆÊñáÁ´†Ôºà‰æãÔºâ]
        - ÊñáÁ´†„Å´‰∏çË∂≥„Åó„Å¶„ÅÑ„ÇãÊÉÖÂ†±„ÇÑË©≥Á¥∞„ÇíËøΩÂä†„Åó„Åæ„Åô„ÄÇ
        - Ë™≠ËÄÖ„ÅåÁêÜËß£„Åó„ÇÑ„Åô„ÅÑ„Çà„ÅÜ„Å´„ÄÅÂøÖË¶Å„Å™ËÉåÊôØÊÉÖÂ†±„ÇÑÂÖ∑‰Ωì‰æã„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ
        - ÊñáÁ´†„ÅÆÂÜÖÂÆπ„ÇíÂÖÖÂÆü„Åï„Åõ„Çã„Åü„ÇÅ„Å´„ÄÅÈñ¢ÈÄ£„Åô„ÇãÊÉÖÂ†±„ÇíË£úË∂≥„Åó„Åæ„Åô„ÄÇ

    # „É¨„Çπ„Éù„É≥„ÇπÂÆöÁæ©
    - ÂøÖ„ÅöÂêÑ„É¢„Éº„Éâ„ÅÆ„É¨„Çπ„Éù„É≥„Çπ„ÅÆÂÆöÁæ©„Å´Âæì„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
    - ÂøÖ„ÅöÂÆöÁæ©„Åó„Åü„É¨„Çπ„Éù„É≥„ÇπÈÄö„Çä„Å´ËøîÁ≠î„Åó„ÄÅ„Åù„ÅÆ„É¨„Çπ„Éù„É≥„Çπ„ÅÆ„Åø„ÇíËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
    - ÂÖ®„Å¶„ÅÆ„É¢„Éº„Éâ„Å´„Åä„ÅÑ„Å¶„ÄÅ**ÂøÖ„Åö**Â∞ÇÈñÄÁî®Ë™û„ÉªÂõ∫ÊúâÂêçË©û„Å™„Å©„Åå„ÅÇ„Çå„Å∞ÊúÄÊñ∞ÊÉÖÂ†±„ÇíGoogleSearch„ÇíÂà©Áî®„Åó„Å¶ÂèñÂæó„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

    „Åù„Çå„Åß„ÅØ„Åì„Çå„Åã„Çâ„ÄÅ„ÅÇ„Å™„Åü„ÅÆÂΩπÂâ≤„ÅØ„ÄåDocsReview.AI„Äç„ÅÆ‰∏ÄÈÉ®„Å®„Åó„Å¶„ÄÅ„É¶„Éº„Ç∂„Éº„Åã„Çâ„ÅÆÊåáÁ§∫„Å´Âæì„Å£„Å¶ÊñáÁ´†„ÇíÂá¶ÁêÜ„Åô„Çã„Åì„Å®„Åß„Åô„ÄÇ
    ÂøÖ„Åö„ÄÅ‰∏äË®ò„ÅßË™¨Êòé„Åó„ÅüÈ†ÖÁõÆ„Å´Âæì„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ‰ªäÂõû„ÅÆÊåáÁ§∫„ÅØ„Äå{ai_mode}„Äç„Åß„Åô„ÄÇ
    
    """


def split_response_into_parts(response: str, max_length: int = 1800) -> list[str]:
    """Split a long response into multiple parts for Discord messages
    
    Args:
        response: The full response text
        max_length: Maximum length per part (leaving room for buttons and formatting)
        
    Returns:
        List of response parts
    """
    if len(response) <= max_length:
        return [response]
    
    parts = []
    remaining = response
    
    while remaining:
        if len(remaining) <= max_length:
            parts.append(remaining)
            break
        
        # Find a good break point
        break_point = max_length
        
        # Try to break at a sentence boundary
        last_period = remaining[:max_length].rfind('„ÄÇ')
        last_newline = remaining[:max_length].rfind('\n')
        last_space = remaining[:max_length].rfind(' ')
        
        # Choose the best break point
        if last_period > max_length * 0.7:
            break_point = last_period + 1
        elif last_newline > max_length * 0.7:
            break_point = last_newline + 1
        elif last_space > max_length * 0.7:
            break_point = last_space + 1
        
        parts.append(remaining[:break_point].strip())
        remaining = remaining[break_point:].strip()
    
    return parts


def enhance_user_input_for_search(user_input: str) -> str:
    """Enhance user input to provide context for search usage decisions"""
    # Simply return the input - let Sabo-sensei respond based on the system prompt
    # The mood determination will be handled by external logic or specific triggers
    return user_input


def generate_response(user_input: str, ai_mode: str) -> str:
    """Generate AI response for given input using Gemini API
    
    Args:
        user_input: The user's message text
        max_length: Maximum length of response (default 2000 for compatibility)
        
    Returns:
        The generated response text (full length, splitting handled elsewhere)
        
    Raises:
        Exception: If Gemini API call fails
    """
    try:
        client = setup_gemini_client()
        
        # Enhance user input to encourage search usage
        enhanced_input = enhance_user_input_for_search(user_input)
        logger.info(f"Processing user query: {user_input[:100]}...")
        
        model = gemini_model
        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(text=enhanced_input),
                ],
            ),
        ]
        
        # Configure Google Search tool
        google_search = types.GoogleSearch()
        tools = [
            types.Tool(google_search=google_search),
        ]
        
        generate_content_config = types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(
                thinking_budget=-1,
            ),
            tools=tools,
            response_mime_type="text/plain",
            system_instruction=[
                types.Part.from_text(text=get_system_prompt(ai_mode)),
            ],
        )
        
        logger.info(f"Generating response for query: {enhanced_input[:100]}...")
        
        # Try non-streaming API first for better grounding support
        try:
            response = client.models.generate_content(
                model=model,
                contents=contents,
                config=generate_content_config,
            )
            
            complete_response = ""
            search_used = False
            
            if response.candidates:
                for candidate in response.candidates:
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            if part.text:
                                complete_response += part.text
                    
                    # Check for grounding metadata
                    if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                        search_used = True
                        logger.info("Google Search grounding detected in non-streaming response")
                        
                        # Log search queries and results
                        if hasattr(candidate.grounding_metadata, 'search_entry_point'):
                            logger.info(f"Search entry point: {candidate.grounding_metadata.search_entry_point}")
                        
                        # Log grounding chunks for URL extraction
                        if (hasattr(candidate.grounding_metadata, 'grounding_chunks') and 
                            candidate.grounding_metadata.grounding_chunks):
                            for chunk in candidate.grounding_metadata.grounding_chunks:
                                if hasattr(chunk, 'web') and chunk.web:
                                    if hasattr(chunk.web, 'uri'):
                                        logger.info(f"Grounding URL found: {chunk.web.uri}")
                                    if hasattr(chunk.web, 'title'):
                                        logger.info(f"Grounding source: {chunk.web.title}")
            
            if complete_response:
                # Check if URLs are included in the response
                url_indicators = ["http://", "https://", ".com", ".org", ".net", ".jp"]
                urls_included = any(indicator in complete_response for indicator in url_indicators)
                
                # Log Google Search usage
                if search_used:
                    logger.info("‚úÖ Response generated with Google Search grounding")
                    if urls_included:
                        logger.info("üîó URLs/links included in response")
                    else:
                        logger.warning("‚ö†Ô∏è No URLs found in response despite grounding")
                else:
                    logger.info("üìù Response generated without Google Search")
                
                return complete_response
                
        except Exception as streaming_error:
            logger.warning(f"Non-streaming API failed, falling back to streaming: {streaming_error}")
        
        # Fallback to streaming API
        response_parts = []
        grounding_metadata = []
        search_used = False
        
        for chunk in client.models.generate_content_stream(
            model=model,
            contents=contents,
            config=generate_content_config,
        ):
            if chunk.text:
                response_parts.append(chunk.text)
            
            # Check for grounding metadata
            if hasattr(chunk, 'candidates') and chunk.candidates:
                for candidate in chunk.candidates:
                    if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                        grounding_metadata.append(candidate.grounding_metadata)
                        search_used = True
                        logger.info("Google Search grounding detected in streaming response")
        
        complete_response = ''.join(response_parts)
        
        if not complete_response:
            raise ValueError("Empty response from Gemini API")
        
        # Check if URLs are included in the response
        url_indicators = ["http://", "https://", ".com", ".org", ".net", ".jp"]
        urls_included = any(indicator in complete_response for indicator in url_indicators)
        
        # Log Google Search usage for streaming fallback
        if search_used:
            logger.info(f"‚úÖ Streaming response generated with Google Search grounding. Metadata count: {len(grounding_metadata)}")
            if urls_included:
                logger.info("üîó URLs/links included in streaming response")
            else:
                logger.warning("‚ö†Ô∏è No URLs found in streaming response despite grounding")
        else:
            logger.info("üìù Streaming response generated without Google Search")
        
        # Return the full response without truncation
        # Splitting will be handled by the response manager
        return complete_response
        
    except Exception as e:
        logger.error(f"Failed to generate response from Gemini: {str(e)}")
        raise


def generate_response_with_error_handling(user_input: str, ai_mode: str) -> dict:
    """Generate response with structured error handling
    
    Args:
        user_input: The user's message text
        
    Returns:
        Dictionary with 'success', 'response', and optional 'error' keys
    """
    try:
        response = generate_response(user_input, ai_mode)
        
        return {
            'success': True,
            'response': response
        }
    except ValueError as e:
        if "GEMINI_API_KEY" in str(e):
            return {
                'success': False,
                'error': 'api_key_missing',
                'message': 'Gemini API key is not configured'
            }
        else:
            return {
                'success': False,
                'error': 'invalid_response',
                'message': str(e)
            }
    except Exception as e:
        logger.error(f"Unexpected error in Gemini integration: {str(e)}")
        return {
            'success': False,
            'error': 'api_error',
            'message': 'Failed to generate response from Gemini API'
        }