import os
from dotenv import load_dotenv
import logging
from google import genai
from google.genai import types
from google.genai.types import (
    GenerateContentConfig,
    GoogleSearch,
    Tool,
)

import models.ai_model

dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env')
load_dotenv(dotenv_path=dotenv_path)

ai_modes = models.ai_model

logger = logging.getLogger(__name__)


gemini_model = str(os.environ.get("GEMINI_MODEL"))
gemini_api_key = str(os.environ.get("GEMINI_API_KEY"))

def setup_gemini_client():
    # if not gemini_api_key:
    #     raise ValueError("GEMINI_API_KEY environment variable is not set")
    
    return genai.Client(api_key=gemini_api_key)


def get_system_prompt(ai_mode: str) -> str:
    if ai_mode in str(ai_modes):
        raise ValueError("ai_mode is required")

    return f"""
    ã‚ãªãŸã¯DocsReview.AIã¨ã„ã†AIã‚µãƒ¼ãƒ“ã‚¹ã®ä¸€éƒ¨ã§ã™ã€‚

    å—ã‘ã¨ã£ãŸæ–‡ç« ã‚’ã€Œãƒ¬ãƒ“ãƒ¥ãƒ¼(review), ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯(factcheck), è¦ç´„(summary), æ·»å‰Š(collection), è¿½åŠ (addiction)ã€ã®é …ç›®ã§ãã‚Œãžã‚ŒæŒ‡ç¤ºã‚’å—ã‘ãŸé€šã‚Šã«å‡¦ç†ã—ã¦ãã ã•ã„ã€‚
    ã“ã‚Œã‚’`ai_mode`ã¨å‘¼ã³ã¾ã™ã€‚ã“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€å¾Œã®éƒ¨åˆ†ã§`ai_mode`ã‚’æŒ‡ç¤ºã—ã¾ã™ã€‚å¿…ãšãã‚Œã«å¾“ã£ã¦ãã ã•ã„ã€‚

    # å„ãƒ¢ãƒ¼ãƒ‰èª¬æ˜Ž

    1. ãƒ¬ãƒ“ãƒ¥ãƒ¼(review):
        - ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼š[ãƒ¬ãƒ“ãƒ¥ãƒ¼æ¦‚è¦]â†’[æ”¹å–„ç‚¹ã®æç¤º]
        - æ–‡ç« å…¨ä½“ã®å†…å®¹ã‚’ç†Ÿèª­ã—ã€å†…å®¹ã®æ­£ç¢ºæ€§ã‚„è«–ç†æ€§ã‚’ç¢ºèªã—ã¾ã™ã€‚
        - æ–‡ç« ã®æ§‹æˆã‚„è¡¨ç¾ã«ã¤ã„ã¦ã‚‚è©•ä¾¡ã—ã€å¿…è¦ã«å¿œã˜ã¦æ”¹å–„ç‚¹ã‚’æŒ‡æ‘˜ã—ã¾ã™ã€‚
        - å…·ä½“çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã€ã©ã®éƒ¨åˆ†ãŒè‰¯ã„ã‹ã€ã©ã®éƒ¨åˆ†ãŒæ”¹å–„ãŒå¿…è¦ã‹ã‚’æ˜Žç¢ºã«ã—ã¾ã™ã€‚ 

    2. ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯(factcheck):
        - ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼š [ä¸æ­£ç¢ºãªæƒ…å ±ã®ä¸€è¦§]â†’[è¨‚æ­£å¾Œã®æ–‡ç« ]
        - æ–‡ç« å†…ã®äº‹å®Ÿã‚„ãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºæ€§ã‚’ç¢ºèªã—ã¾ã™ã€‚
        - å°‚é–€ç”¨èªžã‚„å›ºæœ‰åè©žãªã©ã€ä¿¡é ¼æ€§ã®é«˜ã„æƒ…å ±æºã‚’å‚ç…§ã—ã€äº‹å®Ÿç¢ºèªã‚’è¡Œã„ã¾ã™ã€‚

    3. è¦ç´„(summary):
        - ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼š [è¦ç´„æ¡ˆã‚’æç¤º] â†’ [è¦ç´„å¾Œã®æ–‡ç« ï¼ˆä¾‹ï¼‰]
        - æ–‡ç« ã®è¦ç‚¹ã‚’æŠ½å‡ºã—ã€ç°¡æ½”ã«ã¾ã¨ã‚ã¾ã™ã€‚
        - é‡è¦ãªæƒ…å ±ã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚ã€èª­è€…ãŒç†è§£ã—ã‚„ã™ã„å½¢ã§è¦ç´„ã—ã¾ã™ã€‚
        - è¦ç´„ã¯ã€å…ƒã®æ–‡ç« ã®å†…å®¹ã‚’æãªã‚ãªã„ã‚ˆã†ã«æ³¨æ„ã—ã€å¿…è¦ãªæƒ…å ±ã‚’ç¶²ç¾…ã—ã¾ã™ã€‚

    4. æ·»å‰Š(collection):
        - ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼š [æ·»å‰Šæ¡ˆã‚’æç¤º] â†’ [æ·»å‰Šå¾Œã®æ–‡ç« ï¼ˆä¾‹ï¼‰]
        - æ–‡ç« ã®æ–‡æ³•ã‚„è¡¨ç¾ã®èª¤ã‚Šã‚’æŒ‡æ‘˜ã—ã¾ã™ã€‚
        - èª¤å­—è„±å­—ã€æ–‡æ³•ãƒŸã‚¹ã€è¡¨ç¾ã®ä¸è‡ªç„¶ã•ã‚’ä¿®æ­£ã—ã¾ã™ã€‚
        - æ–‡ç« ã®æµã‚Œã‚„è«–ç†æ€§ã‚’å‘ä¸Šã•ã•ã›ã‚‹ãŸã‚ã®ææ¡ˆã‚‚è¡Œã„ã¾ã™ã€‚

    5. è¿½åŠ (addiction):
        - ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼š [è¿½åŠ æ¡ˆã‚’æç¤º] â†’ [è¿½åŠ å¾Œã®æ–‡ç« ï¼ˆä¾‹ï¼‰]
        - æ–‡ç« ã«ä¸è¶³ã—ã¦ã„ã‚‹æƒ…å ±ã‚„è©³ç´°ã‚’è¿½åŠ ã—ã¾ã™ã€‚
        - èª­è€…ãŒç†è§£ã—ã‚„ã™ã„ã‚ˆã†ã«ã€å¿…è¦ãªèƒŒæ™¯æƒ…å ±ã‚„å…·ä½“ä¾‹ã‚’æä¾›ã—ã¾ã™ã€‚
        - æ–‡ç« ã®å†…å®¹ã‚’å……å®Ÿã•ã›ã‚‹ãŸã‚ã«ã€é–¢é€£ã™ã‚‹æƒ…å ±ã‚’è£œè¶³ã—ã¾ã™ã€‚

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å®šç¾©
    - å¿…ãšå„ãƒ¢ãƒ¼ãƒ‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å®šç¾©ã«å¾“ã£ã¦ãã ã•ã„ã€‚
    - å¿…ãšå®šç¾©ã—ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹é€šã‚Šã«è¿”ç­”ã—ã€ãã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
    - å…¨ã¦ã®ãƒ¢ãƒ¼ãƒ‰ã«ãŠã„ã¦ã€**å¿…ãš**å°‚é–€ç”¨èªžãƒ»å›ºæœ‰åè©žãªã©ãŒã‚ã‚Œã°æœ€æ–°æƒ…å ±ã‚’GoogleSearchã‚’åˆ©ç”¨ã—ã¦å–å¾—ã—ã¦ãã ã•ã„ã€‚

    ãã‚Œã§ã¯ã“ã‚Œã‹ã‚‰ã€ã‚ãªãŸã®å½¹å‰²ã¯ã€ŒDocsReview.AIã€ã®ä¸€éƒ¨ã¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æŒ‡ç¤ºã«å¾“ã£ã¦æ–‡ç« ã‚’å‡¦ç†ã™ã‚‹ã“ã¨ã§ã™ã€‚
    å¿…ãšã€ä¸Šè¨˜ã§èª¬æ˜Žã—ãŸé …ç›®ã«å¾“ã£ã¦ãã ã•ã„ã€‚ä»Šå›žã®æŒ‡ç¤ºã¯ã€Œ{ai_mode}ã€ã§ã™ã€‚
    
    """


def split_response_into_parts(response: str, max_length: int = 1800) -> list[str]:
    """Split a long response into multiple parts for Discord messages
    
    Args:
        response: The full response text
        max_length: Maximum length per part (leaving room for buttons and formatting)
        
    Returns:
        List of response parts
    """
    if len(response) <= max_length:
        return [response]
    
    parts = []
    remaining = response
    
    while remaining:
        if len(remaining) <= max_length:
            parts.append(remaining)
            break
        
        # Find a good break point
        break_point = max_length
        
        # Try to break at a sentence boundary
        last_period = remaining[:max_length].rfind('ã€‚')
        last_newline = remaining[:max_length].rfind('\n')
        last_space = remaining[:max_length].rfind(' ')
        
        # Choose the best break point
        if last_period > max_length * 0.7:
            break_point = last_period + 1
        elif last_newline > max_length * 0.7:
            break_point = last_newline + 1
        elif last_space > max_length * 0.7:
            break_point = last_space + 1
        
        parts.append(remaining[:break_point].strip())
        remaining = remaining[break_point:].strip()
    
    return parts


def enhance_user_input_for_search(user_input: str) -> str:
    """Enhance user input to provide context for search usage decisions"""
    # Simply return the input - let Sabo-sensei respond based on the system prompt
    # The mood determination will be handled by external logic or specific triggers
    return user_input


def generate_response(user_input: str, ai_mode: str) -> str:
    """Generate AI response for given input using Gemini API
    
    Args:
        user_input: The user's message text
        max_length: Maximum length of response (default 2000 for compatibility)
        
    Returns:
        The generated response text (full length, splitting handled elsewhere)
        
    Raises:
        Exception: If Gemini API call fails
    """
    try:
        client = setup_gemini_client()
        
        # Enhance user input to encourage search usage
        enhanced_input = enhance_user_input_for_search(user_input)
        logger.info(f"Processing user query: {user_input[:100]}...")
        
        model = gemini_model
        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(text=enhanced_input),
                ],
            ),
        ]
        
        # Configure Google Search tool
        google_search = types.GoogleSearch()
        tools = [
            types.Tool(google_search=google_search),
        ]
        
        generate_content_config = types.GenerateContentConfig(
            thinking_config=types.ThinkingConfig(
                thinking_budget=-1,
            ),
            tools=tools,
            response_mime_type="text/plain",
            system_instruction=[
                types.Part.from_text(text=get_system_prompt(ai_mode)),
            ],
        )
        
        logger.info(f"Generating response for query: {enhanced_input[:100]}...")
        
        # Try non-streaming API first for better grounding support
        try:
            response = client.models.generate_content(
                model=model,
                contents=contents,
                config=generate_content_config,
            )
            
            complete_response = ""
            search_used = False
            
            if response.candidates:
                for candidate in response.candidates:
                    if candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            if part.text:
                                complete_response += part.text
                    
                    # Check for grounding metadata
                    if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                        search_used = True
                        logger.info("Google Search grounding detected in non-streaming response")
                        
                        # Log search queries and results
                        if hasattr(candidate.grounding_metadata, 'search_entry_point'):
                            logger.info(f"Search entry point: {candidate.grounding_metadata.search_entry_point}")
                        
                        # Log grounding chunks for URL extraction
                        if (hasattr(candidate.grounding_metadata, 'grounding_chunks') and 
                            candidate.grounding_metadata.grounding_chunks):
                            for chunk in candidate.grounding_metadata.grounding_chunks:
                                if hasattr(chunk, 'web') and chunk.web:
                                    if hasattr(chunk.web, 'uri'):
                                        logger.info(f"Grounding URL found: {chunk.web.uri}")
                                    if hasattr(chunk.web, 'title'):
                                        logger.info(f"Grounding source: {chunk.web.title}")
            
            if complete_response:
                # Check if URLs are included in the response
                url_indicators = ["http://", "https://", ".com", ".org", ".net", ".jp"]
                urls_included = any(indicator in complete_response for indicator in url_indicators)
                
                # Log Google Search usage
                if search_used:
                    logger.info("âœ… Response generated with Google Search grounding")
                    if urls_included:
                        logger.info("ðŸ”— URLs/links included in response")
                    else:
                        logger.warning("âš ï¸ No URLs found in response despite grounding")
                else:
                    logger.info("ðŸ“ Response generated without Google Search")
                
                return complete_response
                
        except Exception as streaming_error:
            logger.warning(f"Non-streaming API failed, falling back to streaming: {streaming_error}")
        
        # Fallback to streaming API
        response_parts = []
        grounding_metadata = []
        search_used = False
        
        for chunk in client.models.generate_content_stream(
            model=model,
            contents=contents,
            config=generate_content_config,
        ):
            if chunk.text:
                response_parts.append(chunk.text)
            
            # Check for grounding metadata
            if hasattr(chunk, 'candidates') and chunk.candidates:
                for candidate in chunk.candidates:
                    if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                        grounding_metadata.append(candidate.grounding_metadata)
                        search_used = True
                        logger.info("Google Search grounding detected in streaming response")
        
        complete_response = ''.join(response_parts)
        
        if not complete_response:
            raise ValueError("Empty response from Gemini API")
        
        # Check if URLs are included in the response
        url_indicators = ["http://", "https://", ".com", ".org", ".net", ".jp"]
        urls_included = any(indicator in complete_response for indicator in url_indicators)
        
        # Log Google Search usage for streaming fallback
        if search_used:
            logger.info(f"âœ… Streaming response generated with Google Search grounding. Metadata count: {len(grounding_metadata)}")
            if urls_included:
                logger.info("ðŸ”— URLs/links included in streaming response")
            else:
                logger.warning("âš ï¸ No URLs found in streaming response despite grounding")
        else:
            logger.info("ðŸ“ Streaming response generated without Google Search")
        
        # Return the full response without truncation
        # Splitting will be handled by the response manager
        return complete_response
        
    except Exception as e:
        logger.error(f"Failed to generate response from Gemini: {str(e)}")
        raise


def generate_response_with_error_handling(user_input: str, ai_mode: str) -> dict:
    """Generate response with structured error handling
    
    Args:
        user_input: The user's message text
        
    Returns:
        Dictionary with 'success', 'response', and optional 'error' keys
    """
    try:
        response = generate_response(user_input, ai_mode)
        
        return {
            'success': True,
            'response': response
        }
    except ValueError as e:
        if "GEMINI_API_KEY" in str(e):
            return {
                'success': False,
                'error': 'api_key_missing',
                'message': 'Gemini API key is not configured'
            }
        else:
            return {
                'success': False,
                'error': 'invalid_response',
                'message': str(e)
            }
    except Exception as e:
        logger.error(f"Unexpected error in Gemini integration: {str(e)}")
        return {
            'success': False,
            'error': 'api_error',
            'message': 'Failed to generate response from Gemini API'
        }